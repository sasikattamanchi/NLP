{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e14252a",
   "metadata": {},
   "source": [
    "Implementation of model to generate word2vec embeddings for telugu language. The model is trained on a corpus of wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d18098",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b5a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ede4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9bcc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ee784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ffb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d443a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870bad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bdb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe006b",
   "metadata": {},
   "source": [
    "## Generate skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample sentence to test the code\n",
    "sentences = ['This is a notebook'.lower().split(), 'Another good notebook exists somewhere else as far as I know'.lower().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_word_contextword_pairs generate_word_contextword_pairs(sentence, n_of_ngram=3):\n",
    "#     n = n_of_ngram\n",
    "#     indx = int((n_of_ngram-1)/2)\n",
    "#     all_word_context_pairs = []\n",
    "#     for i in np.arange(len(sentence)-n + 1):\n",
    "#         all_word_context_pairs.append([(sentence[i+indx], context_word) for context_word in sentence[i:i+indx]])\n",
    "#         all_word_context_pairs.append([(sentence[i+indx], context_word) for context_word in sentence[i+indx+1:i+n]])\n",
    "    \n",
    "#     return list(chain.from_iterable(all_word_context_pairs))\n",
    "\n",
    "# def generate_word_contextword_pairs(sentence, window_size=5): \n",
    "#     all_word_context_pairs = []\n",
    "#     for i in np.arange(window_size, len(sentence)-window_size):\n",
    "#         all_word_context_pairs.append([(sentence[i], context_word) for context_word in sentence[i-window_size:i]])\n",
    "#         all_word_context_pairs.append([(sentence[i], context_word) for context_word in sentence[i+1:i+window_size+1]])\n",
    "    \n",
    "#     return list(chain.from_iterable(all_word_context_pairs))\n",
    "\n",
    "def generate_word_contextword_pairs(sentence, window_size=5): \n",
    "    all_word_context_pairs = []\n",
    "    \n",
    "    for i, word in enumerate(sentence):\n",
    "#         win_size = np.random.randint(low = 1, high = window_size+1 )\n",
    "        win_size = window_size\n",
    "        contexts = sentence[i-win_size:i] + sentence[i+1:i+win_size+1]\n",
    "        all_word_context_pairs.append([(word, context_word) for context_word in contexts])\n",
    "    return list(chain.from_iterable(all_word_context_pairs))\n",
    "\n",
    "def generate_word_contextword_pairs_dynamic(sentence, window_size=5): \n",
    "    # window size is dynamic. window_size parameter is the maximum allowed window_size.\n",
    "    # so for each word, win_size is uniformly sampled from [1, window_size]\n",
    "    # Ref: Goldberg and Levy 2014 \n",
    "\n",
    "    all_word_context_pairs = []\n",
    "    \n",
    "    for i, word in enumerate(sentence):\n",
    "        win_size = np.random.randint(low = 1, high = window_size+1 )\n",
    "#         win_size = window_size\n",
    "        contexts = sentence[i-win_size:i] + sentence[i+1:i+win_size+1]\n",
    "        all_word_context_pairs.append([(word, context_word) for context_word in contexts])\n",
    "    return list(chain.from_iterable(all_word_context_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ecd457",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_contextword_pairs_dynamic(sentences[1],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c088101",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_contextword_pairs(sentences[1],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_corpus = open('Data/te_wiki_cleaned_dedup.txt').read().splitlines()\n",
    "# Remove digits\n",
    "te_corpus = [re.sub(r'\\d+', '',line).strip() for line in te_corpus] \n",
    "# Remove all space/tab/newline/half-space characters\n",
    "te_corpus = [re.sub(r'\\u200c', ' ', line) for line in te_corpus]\n",
    "print(f'Length of corpus: {len(te_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4302fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(te_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d82a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "te_corpus = [line.translate(str.maketrans('', '', string.punctuation)) for line in te_corpus] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty lines and lowercasing\n",
    "te_corpus = [line.lower() for line in te_corpus if line] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove lines with less than 15 words\n",
    "te_corpus = [line.strip() for line in te_corpus if len(line.strip().split()) >= 15]\n",
    "print(f'Length of corpus: {len(te_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d1ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "te_corpus = [line.strip().split() for line in te_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b623db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(te_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq = dict(Counter(list(chain.from_iterable(te_corpus))))\n",
    "print(f'Number of tokens: {len(unigram_freq)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7560641",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occurrences = 50\n",
    "unigram_freq_filtered ={}\n",
    "[unigram_freq_filtered.update({key:unigram_freq[key]}) for key in unigram_freq.keys() if unigram_freq[key]>min_occurrences]\n",
    "print(f'Number of tokens: {len(unigram_freq_filtered)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8591ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(unigram_freq_filtered.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2da4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "vocab = list(unigram_freq_filtered.keys())\n",
    "# vocab.insert(0, '<unk>')\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd044a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries to help convert between index and word\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "indx_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c225f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle low freq words\n",
    "te_corpus_N = []\n",
    "for sentence in te_corpus:\n",
    "    sent_N = []\n",
    "    for word in sentence:\n",
    "        if word in vocab:\n",
    "            sent_N.append(word) \n",
    "#         else:\n",
    "#             sent_N.append('<unk>') # remove them; ref: Goldberg and Levy 2014\n",
    "    te_corpus_N.append(sent_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47953d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(te_corpus), len(te_corpus_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffbf846",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_freq = torch.Tensor(list(Counter(list(chain.from_iterable(te_corpus_N))).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dist = vocab_freq**0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf799142",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dist = sampling_dist/sampling_dist.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(chain.from_iterable([generate_word_contextword_pairs(sentence,5) for sentence in te_corpus_N]))\n",
    "# data = list(chain.from_iterable([generate_word_contextword_pairs_dynamic(sentence,5) for sentence in te_corpus_N]))\n",
    "print(f'A few word-context pairs: {data[0:10]}')\n",
    "data = [(word_to_idx[a], word_to_idx[b]) for a,b in data]\n",
    "print(f'A few word-context pairs interms of vocab indices: {data[0:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c21a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(data, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebaa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNegativeSampling(nn.Module):\n",
    "    \n",
    "    def __init__(self,embedding_size, vocab_size, num_neg_samples=5, sampling_weights=None):\n",
    "        super(SkipGramNegativeSampling, self).__init__()\n",
    "        self.input_embedding = nn.Embedding(vocab_size,embedding_size)\n",
    "        self.output_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "#         r = 0.01/embedding_size\n",
    "#         r = 5\n",
    "#         torch.nn.init.uniform_(self.input_embedding.weight, -r, r)\n",
    "#         torch.nn.init.uniform_(self.output_embedding.weight, -r, r)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size      \n",
    "        self.num_neg_samples = num_neg_samples\n",
    "        if sampling_weights is not None:\n",
    "            self.sampling_weights = sampling_weights\n",
    "        else:\n",
    "            self.sampling_weights = torch.ones(self.vocab_size)/self.vocab_size\n",
    "            \n",
    "        \n",
    "    def forward(self, inputs, outputs):\n",
    "        # Dont need forward. Computing loss dirictly is simpler\n",
    "#         raise NotImplementedError\n",
    "        out = self.input_embedding(inputs)* self.output_embedding(outputs)\n",
    "#         for _ in out.shape[1:]:\n",
    "#             out= out.sum(1)\n",
    "#         return out\n",
    "        return out.sum(1)\n",
    "    \n",
    "    def negativeSampling(self, num_samples):        \n",
    "        # returns indices of sampled words\n",
    "        return torch.multinomial(self.sampling_weights, num_samples, replacement=True)\n",
    "    \n",
    "    def loss(self, inputs, outputs, negative_samples):\n",
    "        input_em = self.input_embedding(inputs)\n",
    "        output_em = self.output_embedding(outputs)\n",
    "        neg_samples_em = self.output_embedding(negative_samples)\n",
    "\n",
    "        loss_val_term1 = F.logsigmoid(torch.sum(input_em* output_em, dim=1))\n",
    "        loss_val_term2 = torch.sum(F.logsigmoid(-torch.sum((input_em.unsqueeze(1).repeat((1,self.num_neg_samples,1))*neg_samples_em), dim=2)), dim=1)\n",
    "        \n",
    "        return -torch.sum(loss_val_term1 + loss_val_term2)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81581904",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = te_corpus_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(corpus,'teWikiCorpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db0473",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGramNegativeSampling(embedding_size= 100, vocab_size=vocab_size, num_neg_samples= 5, sampling_weights=sampling_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[:1000000]\n",
    "# NUM_BATCHES=100\n",
    "LEARNING_RATE =0.01\n",
    "NUM_EPOCHS = 100\n",
    "LOG_INTERVAL = 10\n",
    "\n",
    "# Free memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "start = time.time() # timer\n",
    "\n",
    "# training\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "loss_values =[]\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "# dataloader = torch.utils.data.DataLoader(data, batch_size=int(len(data)/NUM_BATCHES), shuffle=True)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[ 100, 150, 200], gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "#                                                 max_lr=0.025, \n",
    "#                                                 steps_per_epoch=len(dataloader), \n",
    "# #                                                 anneal_strategy = 'linear',\n",
    "#                                                 epochs=n_epochs\n",
    "#                                                )\n",
    "scheduler = None\n",
    "\n",
    "\n",
    "for epoch in trange(NUM_EPOCHS):\n",
    "    loss_total = 0\n",
    "    \n",
    "    for word_indx, context_word_indx in dataloader:\n",
    "         \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()     \n",
    "\n",
    "        # compute loss\n",
    "        inp = word_indx.to(device)\n",
    "        outp = context_word_indx.to(device)\n",
    "        negative_samples = model.negativeSampling(model.num_neg_samples*inp.shape[0]) # batch size = inputs.shape[0]\n",
    "        negative_samples = negative_samples.reshape(inp.shape[0],model.num_neg_samples)\n",
    "        negative_samples = negative_samples.to(device)\n",
    "        \n",
    "        loss = model.loss(inp, outp,negative_samples)\n",
    "        loss_total += loss.item()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "    loss_values.append(loss_total)\n",
    "            \n",
    "    if epoch%LOG_INTERVAL ==0:\n",
    "        print(\"Epoch \" + str(epoch) + \" done. Loss: \" + str(loss_total))\n",
    "#         torch.save(model, 'saved_model.pkl')\n",
    "        \n",
    "print(\"Epoch \" + str(epoch) + \" done. Loss: \" + str(loss_total))\n",
    "elapsed = time.time() - start\n",
    "print(f\"Model trained for {n_epochs} in {elapsed/60: .2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'saved_model_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f47943",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "em = nn.Embedding.from_pretrained(model.input_embedding.weight, freeze=True)\n",
    "word_emb = em.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(w1, w2):\n",
    "    return np.dot(w1,w2)/(np.linalg.norm(w1) * np.linalg.norm(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0cc244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSimilar(word, word_emb, k=5):\n",
    "    word_vec = word_emb[word_to_idx[word]]\n",
    "\n",
    "    sim = np.zeros(word_emb.shape[0])\n",
    "    for i,row in enumerate(word_emb):\n",
    "        sim[i] = cosineSimilarity(word_vec.numpy(), row.numpy())\n",
    "    val,ind = torch.topk(torch.Tensor(sim), k+1)\n",
    "    return [(vocab[ind[i]], val[i].item() )for i in np.arange(1, len(ind))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be190212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# findSimilar('queen', word_emb,10)\n",
    "# word = 'తక్కువ' #'లోపల'#'పాలు'#'సమీప'#'దివ్యాంగుల'#'జైపూర్'\n",
    "# word = 'పంటలు'#'నీరు'#'తక్కువ' #'పాలు'#'అన్న' #'అక్కడ' #'దేశం'\n",
    "# word = 'దక్షిణ'#'తండ్రి'\n",
    "word = 'బంగారు'#'విద్య'#'రాజ్యాంగం'#'ఇథియోపియా'#'బిలియన్ల'#'నదులు'#'యుద్ధం'#'మొక్కలు'#'గాంధీ'#'విజయం'#'అభివృద్ధి'\n",
    "# word = 'ఎనిమిది'#'కుటుంబం'#'ఆలయ'#'సాయంత్రం'#'నక్షత్రం' #'దిగుమతి'#'పాఠశాల'#'వినియోగం'\n",
    "# word = 'గ్రామం'\n",
    "# word = 'లక్ష'#'ప్రముఖ'#'జర్మన్'#'ప్రజలు'#'గుండె'#'మట్టి'#'ఆధిక్యత'#'చెట్లు'#'పెద్ద'#'ఆధారిత'#'ఇతర'#'జిల్లా'#'ఒకటి'#'దూరం' #'కణాలు'\n",
    "# model.input_embedding.requires_grad_(False)\n",
    "findSimilar(word, em, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9630f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_words = ['king','queen', 'daughter', 'son', 'wife', 'husband',  'mother','father', 'this','that']\n",
    "\n",
    "plot_words = ['కింద', 'మీద']\n",
    "\n",
    "ww_idx = [ word_to_idx[word] for word in plot_words ]\n",
    "word_emb1 = em.weight.numpy()[ww_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2,)\n",
    "ww = pca.fit_transform(word_emb1)\n",
    "\n",
    "fig = px.scatter(ww, x=0, y=1)\n",
    "# fig.show()\n",
    "\n",
    "for i, idx in enumerate(ww_idx):\n",
    "    fig.add_annotation(x=ww[i,0],y=ww[i,1], text=indx_to_word[idx])\n",
    "HTML(fig.to_html())\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(em.weight.numpy())\n",
    "# ww = pca.transform(em.weight.numpy())\n",
    "\n",
    "# fig = px.scatter(ww[ww_idx,:], x=0, y=1)\n",
    "# # fig.show()\n",
    "\n",
    "# for idx in ww_idx:\n",
    "#     fig.add_annotation(x=ww[idx,0],y=ww[idx,1], text=indx_to_word[idx])\n",
    "# HTML(fig.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd70a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
