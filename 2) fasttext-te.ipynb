{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Implementation of model to generate fasttext embeddings for telugu language. The model is trained on a corpus of wikipedia articles. \n\nNote: Preprocessing is skipped and the processed corpus from word2vec_sg_te is directly loaded. See the word2vec_sg_te notbook for processing details.","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"%config Completer.use_jedi=False","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:17.878188Z","iopub.execute_input":"2021-12-11T09:23:17.878709Z","iopub.status.idle":"2021-12-11T09:23:17.897912Z","shell.execute_reply.started":"2021-12-11T09:23:17.878618Z","shell.execute_reply":"2021-12-11T09:23:17.897215Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:17.899096Z","iopub.execute_input":"2021-12-11T09:23:17.899533Z","iopub.status.idle":"2021-12-11T09:23:17.908492Z","shell.execute_reply.started":"2021-12-11T09:23:17.899495Z","shell.execute_reply":"2021-12-11T09:23:17.907400Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import trange, tqdm","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:17.909667Z","iopub.execute_input":"2021-12-11T09:23:17.909972Z","iopub.status.idle":"2021-12-11T09:23:17.955305Z","shell.execute_reply.started":"2021-12-11T09:23:17.909935Z","shell.execute_reply":"2021-12-11T09:23:17.954698Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:17.957599Z","iopub.execute_input":"2021-12-11T09:23:17.957836Z","iopub.status.idle":"2021-12-11T09:23:18.389448Z","shell.execute_reply.started":"2021-12-11T09:23:17.957803Z","shell.execute_reply":"2021-12-11T09:23:18.388676Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from itertools import chain\nfrom collections import Counter\nimport string\nimport time\nimport re\nimport gc\nimport random\nimport math","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:18.390745Z","iopub.execute_input":"2021-12-11T09:23:18.390969Z","iopub.status.idle":"2021-12-11T09:23:18.395897Z","shell.execute_reply.started":"2021-12-11T09:23:18.390937Z","shell.execute_reply":"2021-12-11T09:23:18.395243Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:18.397483Z","iopub.execute_input":"2021-12-11T09:23:18.397921Z","iopub.status.idle":"2021-12-11T09:23:18.410303Z","shell.execute_reply.started":"2021-12-11T09:23:18.397826Z","shell.execute_reply":"2021-12-11T09:23:18.409603Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:18.413126Z","iopub.execute_input":"2021-12-11T09:23:18.413342Z","iopub.status.idle":"2021-12-11T09:23:18.418686Z","shell.execute_reply.started":"2021-12-11T09:23:18.413307Z","shell.execute_reply":"2021-12-11T09:23:18.418013Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice ","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:18.420120Z","iopub.execute_input":"2021-12-11T09:23:18.420556Z","iopub.status.idle":"2021-12-11T09:23:18.449020Z","shell.execute_reply.started":"2021-12-11T09:23:18.420493Z","shell.execute_reply":"2021-12-11T09:23:18.448270Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Generate skipgrams","metadata":{}},{"cell_type":"code","source":"\ndef generate_word_contextword_pairs(sentence, window_size=5): \n    all_word_context_pairs = []\n    \n    for i, word in enumerate(sentence):\n#         win_size = np.random.randint(low = 1, high = window_size+1 )\n        win_size = window_size\n        contexts = sentence[i-win_size:i] + sentence[i+1:i+win_size+1]\n        all_word_context_pairs.append([(word, context_word) for context_word in contexts])\n    return list(chain.from_iterable(all_word_context_pairs))\n\ndef generate_word_contextword_pairs_dynamic(sentence, window_size=5): \n    # window size is dynamic. window_size parameter is the maximum allowed window_size.\n    # so for each word, win_size is uniformly sampled from [1, window_size]\n    # Ref: Goldberg and Levy 2014 \n\n    all_word_context_pairs = []\n    \n    for i, word in enumerate(sentence):\n        win_size = np.random.randint(low = 1, high = window_size+1 )\n#         win_size = window_size\n        contexts = sentence[i-win_size:i] + sentence[i+1:i+win_size+1]\n        all_word_context_pairs.append([(word, context_word) for context_word in contexts])\n    return list(chain.from_iterable(all_word_context_pairs))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:18.450455Z","iopub.execute_input":"2021-12-11T09:23:18.450736Z","iopub.status.idle":"2021-12-11T09:23:18.460234Z","shell.execute_reply.started":"2021-12-11T09:23:18.450696Z","shell.execute_reply":"2021-12-11T09:23:18.459180Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Telugu wikipedia corpus","metadata":{}},{"cell_type":"code","source":"te_corpus = torch.load('Data/teWikiCorpus.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:18.461703Z","iopub.execute_input":"2021-12-11T09:23:18.461973Z","iopub.status.idle":"2021-12-11T09:23:21.362087Z","shell.execute_reply.started":"2021-12-11T09:23:18.461935Z","shell.execute_reply":"2021-12-11T09:23:21.361225Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(te_corpus[:10])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:21.363247Z","iopub.execute_input":"2021-12-11T09:23:21.363550Z","iopub.status.idle":"2021-12-11T09:23:21.370653Z","shell.execute_reply.started":"2021-12-11T09:23:21.363509Z","shell.execute_reply":"2021-12-11T09:23:21.369736Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"unigram_freq = Counter(list(chain.from_iterable(te_corpus)))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:21.371872Z","iopub.execute_input":"2021-12-11T09:23:21.372199Z","iopub.status.idle":"2021-12-11T09:23:21.859040Z","shell.execute_reply.started":"2021-12-11T09:23:21.372162Z","shell.execute_reply":"2021-12-11T09:23:21.858081Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# vocab of words\nvocab_w = list(unigram_freq.keys())\nvocab_w.insert(0, '<unk>')\nvocab_w_size = len(vocab_w)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:21.865510Z","iopub.execute_input":"2021-12-11T09:23:21.865762Z","iopub.status.idle":"2021-12-11T09:23:21.870066Z","shell.execute_reply.started":"2021-12-11T09:23:21.865732Z","shell.execute_reply":"2021-12-11T09:23:21.869262Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Generate character ngrams for each word\n# Ref: page 3 of  Enriching Word Vectors with Subword Information\n# https://arxiv.org/pdf/1607.04606.pdf\n\n# with hash\ndef extract_ngrams(str, n1, n2=None):\n    if n2 is None:\n        nv= [n1]\n    else:\n        nv = np.arange(n1, n2)\n\n    str = '<'+str+'>'\n    n_grams =[]\n    for n in nv:\n        for i in np.arange(len(str)-n+1):\n            n_grams.append(hash(str[i:i+n]))\n    n_grams.append(hash(str))\n    return list(set(n_grams))\n\n# without hash\ndef extract_ngrams_nohash(str, n1, n2=None):\n    if n2 is None:\n        nv= [n1]\n    else:\n        nv = np.arange(n1, n2)\n\n    str = '<'+str+'>'\n    n_grams =[]\n    for n in nv:\n        for i in np.arange(len(str)-n+1):\n            n_grams.append(str[i:i+n])\n    n_grams.append(str)\n    return list(set(n_grams))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:21.871470Z","iopub.execute_input":"2021-12-11T09:23:21.871934Z","iopub.status.idle":"2021-12-11T09:23:21.882182Z","shell.execute_reply.started":"2021-12-11T09:23:21.871898Z","shell.execute_reply":"2021-12-11T09:23:21.881461Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(extract_ngrams_nohash('విస్తీర్ణంలో', 4))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:21.883450Z","iopub.execute_input":"2021-12-11T09:23:21.883908Z","iopub.status.idle":"2021-12-11T09:23:21.894982Z","shell.execute_reply.started":"2021-12-11T09:23:21.883872Z","shell.execute_reply":"2021-12-11T09:23:21.894248Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(extract_ngrams_nohash('రెండవ', 3))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:21.896250Z","iopub.execute_input":"2021-12-11T09:23:21.896758Z","iopub.status.idle":"2021-12-11T09:23:21.904096Z","shell.execute_reply.started":"2021-12-11T09:23:21.896722Z","shell.execute_reply":"2021-12-11T09:23:21.903214Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"char_ngrams_dict = {}\nfor word in vocab_w:\n    char_ngrams_dict.update({word:extract_ngrams(word,3,7)})","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:21.905290Z","iopub.execute_input":"2021-12-11T09:23:21.905722Z","iopub.status.idle":"2021-12-11T09:23:22.295554Z","shell.execute_reply.started":"2021-12-11T09:23:21.905682Z","shell.execute_reply":"2021-12-11T09:23:22.294845Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# vocab of character ngrams\nvocab = set(chain.from_iterable(char_ngrams_dict.values()))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:22.296810Z","iopub.execute_input":"2021-12-11T09:23:22.297048Z","iopub.status.idle":"2021-12-11T09:23:22.317850Z","shell.execute_reply.started":"2021-12-11T09:23:22.297014Z","shell.execute_reply":"2021-12-11T09:23:22.317152Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"vocab_freq = torch.Tensor(list(unigram_freq.values()))\nvocab_size  = len(vocab)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:22.319239Z","iopub.execute_input":"2021-12-11T09:23:22.319501Z","iopub.status.idle":"2021-12-11T09:23:22.325309Z","shell.execute_reply.started":"2021-12-11T09:23:22.319457Z","shell.execute_reply":"2021-12-11T09:23:22.324644Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"sampling_dist = vocab_freq**0.75\nsampling_dist = sampling_dist/sampling_dist.sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:22.326677Z","iopub.execute_input":"2021-12-11T09:23:22.327210Z","iopub.status.idle":"2021-12-11T09:23:22.334460Z","shell.execute_reply.started":"2021-12-11T09:23:22.327173Z","shell.execute_reply":"2021-12-11T09:23:22.333711Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# dictionaries to help convert between index and word\nword_to_idx = {word: i for i, word in enumerate(vocab_w)}\nindx_to_word = {i: word for i, word in enumerate(vocab_w)}","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:22.336890Z","iopub.execute_input":"2021-12-11T09:23:22.338063Z","iopub.status.idle":"2021-12-11T09:23:22.346209Z","shell.execute_reply.started":"2021-12-11T09:23:22.338023Z","shell.execute_reply":"2021-12-11T09:23:22.345385Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# dictionaries to help convert between index and character ngrams\nsubword_to_idx = {word: i for i, word in enumerate(vocab)}\nindx_to_subword = {i: word for i, word in enumerate(vocab)}","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:22.347719Z","iopub.execute_input":"2021-12-11T09:23:22.348231Z","iopub.status.idle":"2021-12-11T09:23:22.386482Z","shell.execute_reply.started":"2021-12-11T09:23:22.348194Z","shell.execute_reply":"2021-12-11T09:23:22.385837Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"word_indx_to_char=[]\nfor i in range(len(indx_to_word)):\n#     word_indx_to_char.append(torch.IntTensor([subword_to_idx[subword] for subword in char_ngrams_dict[indx_to_word[i]]]).long())\n    word_indx_to_char.append([subword_to_idx[subword] for subword in char_ngrams_dict[indx_to_word[i]]])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:22.387672Z","iopub.execute_input":"2021-12-11T09:23:22.387896Z","iopub.status.idle":"2021-12-11T09:23:22.448630Z","shell.execute_reply.started":"2021-12-11T09:23:22.387866Z","shell.execute_reply":"2021-12-11T09:23:22.448002Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# data_sh = list(chain.from_iterable([generate_word_contextword_pairs_dynamic(sentence,2) for sentence in corpus]))\nwin_size=2\ndata_sh = list(chain.from_iterable([generate_word_contextword_pairs(sentence,win_size) for sentence in te_corpus]))\ndata = [(word_to_idx[a], word_to_idx[b]) for a,b in data_sh]","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:22.449884Z","iopub.execute_input":"2021-12-11T09:23:22.450133Z","iopub.status.idle":"2021-12-11T09:23:30.997299Z","shell.execute_reply.started":"2021-12-11T09:23:22.450099Z","shell.execute_reply":"2021-12-11T09:23:30.996467Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 1024*4\ndataloader = torch.utils.data.DataLoader(data, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:30.998976Z","iopub.execute_input":"2021-12-11T09:23:30.999234Z","iopub.status.idle":"2021-12-11T09:23:31.004624Z","shell.execute_reply.started":"2021-12-11T09:23:30.999198Z","shell.execute_reply":"2021-12-11T09:23:31.003070Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"def get_inp_and_offsets(inputs):\n    aa = []\n    offsets = [0]\n    for lst in inputs:\n#         tmp = word_indx_to_char[lst].numpy()\n        tmp = word_indx_to_char[lst]\n        aa.append(tmp)\n        offsets.append(len(tmp)+offsets[-1])\n    offsets.pop(-1)\n    return torch.tensor(list(chain.from_iterable(aa))).to(device), torch.tensor(offsets).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:31.006158Z","iopub.execute_input":"2021-12-11T09:23:31.006560Z","iopub.status.idle":"2021-12-11T09:23:31.015562Z","shell.execute_reply.started":"2021-12-11T09:23:31.006523Z","shell.execute_reply":"2021-12-11T09:23:31.014783Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class FastTextTorch(nn.Module):\n    \n    def __init__(self,embedding_size, in_vocab_size,out_vocab_size, num_neg_samples,char_ngrams_dict):\n        super(FastTextTorch, self).__init__()\n        self.input_embedding = nn.EmbeddingBag(in_vocab_size,embedding_size)\n        self.output_embedding = nn.Embedding(out_vocab_size, embedding_size)\n        \n        self.in_vocab_size = in_vocab_size\n        self.out_vocab_size = out_vocab_size\n        self.embedding_size = embedding_size      \n        self.num_neg_samples = num_neg_samples\n        \n    def forward(self, inputs, outputs):        \n        inp, offsets = get_inp_and_offsets(inputs)\n        input_em = self.input_embedding(inp, offsets)\n        output_em = self.output_embedding(outputs)\n        out = input_em*output_em\n        return out.sum(1)\n    \n    def negativeSampling(self, num_samples,sampling_weights):        \n        # returns indices of sampled words\n        return torch.multinomial(sampling_weights, num_samples, replacement=True)\n    \n    def loss(self, inputs, outputs, negative_samples):\n        \n        input_em = self.input_embedding(inputs)\n        output_em = self.output_embedding(outputs)\n        neg_samples_em = self.output_embedding(negative_samples)\n        loss_val_term1 = F.logsigmoid(torch.sum(input_em* output_em, dim=1))\n        loss_val_term2 = torch.sum(F.logsigmoid(-torch.sum((input_em.unsqueeze(1).repeat((1,self.num_neg_samples,1))*neg_samples_em), dim=2)), dim=1)\n        return -torch.sum(loss_val_term1 + loss_val_term2)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:31.017071Z","iopub.execute_input":"2021-12-11T09:23:31.017425Z","iopub.status.idle":"2021-12-11T09:23:31.029474Z","shell.execute_reply.started":"2021-12-11T09:23:31.017380Z","shell.execute_reply":"2021-12-11T09:23:31.028705Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model = FastTextTorch(embedding_size=100, in_vocab_size=vocab_size,out_vocab_size=vocab_w_size, char_ngrams_dict=char_ngrams_dict, num_neg_samples=5)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:31.030560Z","iopub.execute_input":"2021-12-11T09:23:31.031323Z","iopub.status.idle":"2021-12-11T09:23:31.119890Z","shell.execute_reply.started":"2021-12-11T09:23:31.031278Z","shell.execute_reply":"2021-12-11T09:23:31.119174Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:31.121390Z","iopub.execute_input":"2021-12-11T09:23:31.121662Z","iopub.status.idle":"2021-12-11T09:23:32.808629Z","shell.execute_reply.started":"2021-12-11T09:23:31.121627Z","shell.execute_reply":"2021-12-11T09:23:32.807837Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:32.810086Z","iopub.execute_input":"2021-12-11T09:23:32.810332Z","iopub.status.idle":"2021-12-11T09:23:33.361149Z","shell.execute_reply.started":"2021-12-11T09:23:32.810298Z","shell.execute_reply":"2021-12-11T09:23:33.360374Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 1024\nLEARNING_RATE =0.01\nNUM_EPOCHS = 30\nLOG_INTERVAL = 1\n\noptimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\nloss_func = nn.BCEWithLogitsLoss(reduction='sum')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:33.362700Z","iopub.execute_input":"2021-12-11T09:23:33.363154Z","iopub.status.idle":"2021-12-11T09:23:33.372420Z","shell.execute_reply.started":"2021-12-11T09:23:33.363101Z","shell.execute_reply":"2021-12-11T09:23:33.371435Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"len_dataloader = int(np.ceil(len(data)/BATCH_SIZE))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:33.373915Z","iopub.execute_input":"2021-12-11T09:23:33.374421Z","iopub.status.idle":"2021-12-11T09:23:33.382448Z","shell.execute_reply.started":"2021-12-11T09:23:33.374337Z","shell.execute_reply":"2021-12-11T09:23:33.381679Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def getInputVector(inp_word_indx):\n    # inp: batch_size x 1\n    inp_vec = torch.zeros(inp_word_indx.shape[0], len(subword_to_idx))\n    \n    for i,word_i in enumerate(inp_word_indx):\n        inp_vec[i,word_indx_to_char[word_i].long()] = 1\n    \n#     for i,word_i in enumerate(inp):\n#         for subword in char_ngrams_dict[indx_to_word[word_i.item()]]:\n#             inp_vec[i,subword_to_idx[subword]] = 1\n    return inp_vec\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:33.383873Z","iopub.execute_input":"2021-12-11T09:23:33.384608Z","iopub.status.idle":"2021-12-11T09:23:33.392342Z","shell.execute_reply.started":"2021-12-11T09:23:33.384566Z","shell.execute_reply":"2021-12-11T09:23:33.391463Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def get_inp_and_offsets_transform(inputs):\n    aa = []\n    offsets = [0]\n    for lst in inputs:\n#         tmp = word_indx_to_char[lst].numpy()\n        tmp = word_indx_to_char[lst]\n        aa.append(tmp)\n        offsets.append(len(tmp)+offsets[-1])\n    offsets.pop(-1)\n    return (torch.tensor(list(chain.from_iterable(aa))), torch.tensor(offsets))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:33.395640Z","iopub.execute_input":"2021-12-11T09:23:33.395898Z","iopub.status.idle":"2021-12-11T09:23:33.404876Z","shell.execute_reply.started":"2021-12-11T09:23:33.395862Z","shell.execute_reply":"2021-12-11T09:23:33.404173Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Find optimal Learning rate \ndef find_learning_rate(model, dataloader, initial_lr = 1e-5, max_lr = 1e2,  num_lr_finder_steps=100):\n\n\n    # Free memory\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    # training mode\n    model.to(device)\n    model.train()\n\n    optimizer = torch.optim.Adam(model.parameters(), lr = initial_lr)\n    fac= np.exp((1/num_lr_finder_steps)*np.log(max_lr/initial_lr))\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,lr_lambda=lambda epoch: fac**(epoch+1))\n    # scheduler = None\n    iter_dataloader = iter(dataloader)\n    lr_list = []\n    for iteration in trange(num_lr_finder_steps):\n        \n        word_indx, context_word_indx  = next(iter_dataloader)\n        \n        # zero the gradients\n        optimizer.zero_grad()\n        \n        # compute loss\n        outp = context_word_indx.to(device)\n        inp = word_indx.tolist()\n        pred_p = model(inp,outp)\n        loss = loss_func(pred_p, pred_p*0+1)\n        for _ in range(model.num_neg_samples):\n            negative_samples = model.negativeSampling(outp.shape[0], sampling_dist) # batch size = inputs.shape[0]\n            negative_samples = negative_samples.to(device)\n            pred_n = model(inp,negative_samples)\n\n        lr_list.append([scheduler.get_last_lr()[0],loss.item() ])\n        \n        # backward pass\n        loss.backward()\n        \n        # optimization step\n        optimizer.step()\n        \n        # scheduler step\n        scheduler.step()\n        \n    return np.array(lr_list)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:33.406332Z","iopub.execute_input":"2021-12-11T09:23:33.406811Z","iopub.status.idle":"2021-12-11T09:23:33.419776Z","shell.execute_reply.started":"2021-12-11T09:23:33.406769Z","shell.execute_reply":"2021-12-11T09:23:33.418926Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Remember to run lr_finder on just initialised model\nmodel = FastTextTorch(embedding_size=100, in_vocab_size=vocab_size,out_vocab_size=vocab_w_size, char_ngrams_dict=char_ngrams_dict, num_neg_samples=5)\nlr_list = find_learning_rate(model, dataloader, initial_lr=1e-4,max_lr=1e2,num_lr_finder_steps=100)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:33.421174Z","iopub.execute_input":"2021-12-11T09:23:33.421959Z","iopub.status.idle":"2021-12-11T09:23:43.127118Z","shell.execute_reply.started":"2021-12-11T09:23:33.421913Z","shell.execute_reply":"2021-12-11T09:23:43.126352Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"plt.loglog(lr_list[:,0], lr_list[:,1])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:43.128570Z","iopub.execute_input":"2021-12-11T09:23:43.128993Z","iopub.status.idle":"2021-12-11T09:23:44.102127Z","shell.execute_reply.started":"2021-12-11T09:23:43.128952Z","shell.execute_reply":"2021-12-11T09:23:44.101415Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"optimal_lr = 0.005\nLEARNING_RATE = optimal_lr","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:44.103624Z","iopub.execute_input":"2021-12-11T09:23:44.104028Z","iopub.status.idle":"2021-12-11T09:23:44.108285Z","shell.execute_reply.started":"2021-12-11T09:23:44.103988Z","shell.execute_reply":"2021-12-11T09:23:44.107216Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"loss_values =[]\n\ndef train(model, n_epochs):\n    start = time.time()\n    model.train()\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n#     scheduler =None\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n                                                max_lr=LEARNING_RATE, \n                                                steps_per_epoch=len_dataloader, \n                                                anneal_strategy = 'linear',\n                                                epochs=n_epochs,\n                                                div_factor = 10,\n                                                final_div_factor = 1000,\n                                                pct_start = 0.4,\n                                                three_phase = True,\n                                               )\n    \n    for epoch in trange(n_epochs):\n        loss_total = 0\n        \n        # dynamically generated context word pairs\n        data_sh = list(chain.from_iterable([generate_word_contextword_pairs_dynamic(sentence,win_size) for sentence in te_corpus]))\n        data = [(word_to_idx[a], word_to_idx[b]) for a,b in data_sh]\n        dataloader = torch.utils.data.DataLoader(data, batch_size=BATCH_SIZE, shuffle=True) \n        \n       \n        i_dl=0\n        for word_indx, context_word_indx in tqdm(dataloader, total = len(dataloader)):\n\n            # zero the gradients    \n            optimizer.zero_grad()    \n\n            # compute loss\n            outp = context_word_indx.to(device)\n            inp = word_indx.tolist()\n            pred_p = model(inp,outp)\n            loss = loss_func(pred_p, pred_p*0+1)\n            for _ in range(model.num_neg_samples):\n                negative_samples = model.negativeSampling(outp.shape[0], sampling_dist) # batch size = inputs.shape[0]\n                negative_samples = negative_samples.to(device)\n                pred_n = model(inp,negative_samples)\n\n\n            loss += loss_func(pred_n, pred_n*0)\n            \n            loss_total += loss.item()\n            \n            # backward pass\n            loss.backward()\n\n\n            # optimization step\n            optimizer.step()\n            \n            if scheduler is not None:\n                scheduler.step()\n            \n        loss_values.append(loss_total)\n        \n        if epoch%LOG_INTERVAL ==0:\n            print(\"Epoch \" + str(epoch) + \" done. Loss: \" + str(loss_total))\n#             torch.save(model, 'saved_model_ft.pkl')\n    elapsed = time.time() - start\n    print(f\"Model trained for {n_epochs} in {elapsed/60: .2f} minutes\")\n    \n    torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss_values': loss_values,\n            }, f'FastText_te_{n_epochs}_epochs.pt')\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:44.109838Z","iopub.execute_input":"2021-12-11T09:23:44.110292Z","iopub.status.idle":"2021-12-11T09:23:44.127784Z","shell.execute_reply.started":"2021-12-11T09:23:44.110251Z","shell.execute_reply":"2021-12-11T09:23:44.126937Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\nmodel = FastTextTorch(embedding_size=100, in_vocab_size=vocab_size,out_vocab_size=vocab_w_size, char_ngrams_dict=char_ngrams_dict, num_neg_samples=5)\nmodel.to(device)\nmodel = train(model, n_epochs=NUM_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T09:23:44.133620Z","iopub.execute_input":"2021-12-11T09:23:44.134216Z","iopub.status.idle":"2021-12-11T11:06:22.134271Z","shell.execute_reply.started":"2021-12-11T09:23:44.134005Z","shell.execute_reply":"2021-12-11T11:06:22.133406Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"plt.plot(loss_values)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T11:06:22.135795Z","iopub.execute_input":"2021-12-11T11:06:22.136074Z","iopub.status.idle":"2021-12-11T11:06:22.318910Z","shell.execute_reply.started":"2021-12-11T11:06:22.136038Z","shell.execute_reply":"2021-12-11T11:06:22.318183Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"model.to('cpu')\nmodel.eval()\nem = nn.EmbeddingBag.from_pretrained(model.input_embedding.weight, freeze=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-11T11:06:22.320169Z","iopub.execute_input":"2021-12-11T11:06:22.320427Z","iopub.status.idle":"2021-12-11T11:06:22.372512Z","shell.execute_reply.started":"2021-12-11T11:06:22.320383Z","shell.execute_reply":"2021-12-11T11:06:22.371892Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def cosineSimilarity(w1, w2):\n    return np.dot(w1,w2)/(np.linalg.norm(w1) * np.linalg.norm(w2))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T11:06:22.373904Z","iopub.execute_input":"2021-12-11T11:06:22.374187Z","iopub.status.idle":"2021-12-11T11:06:22.378654Z","shell.execute_reply.started":"2021-12-11T11:06:22.374145Z","shell.execute_reply":"2021-12-11T11:06:22.377716Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def findSimilar(word, em, k=5):\n    inp, offsets = get_inp_and_offsets(torch.IntTensor([word_to_idx[word]]))\n    word_vec = em(inp.to('cpu'), offsets.to('cpu'))\n    word_vec = word_vec.detach().numpy().squeeze()\n\n    sim = np.zeros(len(indx_to_word.keys()))\n    for i,row in enumerate(indx_to_word.keys()):\n        inp, offsets = get_inp_and_offsets(torch.IntTensor([row]))\n        row_v = em(inp.to('cpu'), offsets.to('cpu')).detach().numpy().squeeze()\n        sim[i] = cosineSimilarity(word_vec, row_v)\n\n    val,ind = torch.topk(torch.Tensor(sim), k+1)\n#     plt.scatter(np.arange(len(sim)),sim)\n#     print(val)\n#     print(ind)\n#     print(row.shape)\n    return [(indx_to_word[ind[i].item()], val[i].item() )for i in np.arange(1, len(ind))]","metadata":{"execution":{"iopub.status.busy":"2021-12-11T11:06:22.380081Z","iopub.execute_input":"2021-12-11T11:06:22.380339Z","iopub.status.idle":"2021-12-11T11:06:22.391835Z","shell.execute_reply.started":"2021-12-11T11:06:22.380303Z","shell.execute_reply":"2021-12-11T11:06:22.391133Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"findSimilar('దక్షిణ', em, 10)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T11:06:22.392734Z","iopub.execute_input":"2021-12-11T11:06:22.393317Z","iopub.status.idle":"2021-12-11T11:06:24.229040Z","shell.execute_reply.started":"2021-12-11T11:06:22.393279Z","shell.execute_reply":"2021-12-11T11:06:24.228199Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"findSimilar('పంటలు', em, 10)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T11:07:19.741058Z","iopub.execute_input":"2021-12-11T11:07:19.741346Z","iopub.status.idle":"2021-12-11T11:07:21.853635Z","shell.execute_reply.started":"2021-12-11T11:07:19.741314Z","shell.execute_reply":"2021-12-11T11:07:21.852790Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"findSimilar('ఎనిమిది', em, 10)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T11:07:27.521781Z","iopub.execute_input":"2021-12-11T11:07:27.522050Z","iopub.status.idle":"2021-12-11T11:07:29.298019Z","shell.execute_reply.started":"2021-12-11T11:07:27.522019Z","shell.execute_reply":"2021-12-11T11:07:29.297213Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"findSimilar('తక్కువ', em, 10)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T11:07:34.711148Z","iopub.execute_input":"2021-12-11T11:07:34.711816Z","iopub.status.idle":"2021-12-11T11:07:36.720997Z","shell.execute_reply.started":"2021-12-11T11:07:34.711778Z","shell.execute_reply":"2021-12-11T11:07:36.720229Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"findSimilar('గ్రామం', em, 10)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T11:07:41.683988Z","iopub.execute_input":"2021-12-11T11:07:41.684248Z","iopub.status.idle":"2021-12-11T11:07:43.465227Z","shell.execute_reply.started":"2021-12-11T11:07:41.684217Z","shell.execute_reply":"2021-12-11T11:07:43.464495Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
